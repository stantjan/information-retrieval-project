{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f3daf00",
   "metadata": {},
   "source": [
    "## Part B: Information Retrieval (IR) System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e410df90",
   "metadata": {},
   "source": [
    "##### Part 1: Importing Articles Data into Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6056bfb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total articles imported: 1000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Information Retrieval Model\n",
    "\n",
    "This model is created to retrieve information from 1000 articles. There are:\n",
    "100 business articles\n",
    "100 entertainment articles\n",
    "100 food articles\n",
    "100 graphics articles\n",
    "100 historical articles\n",
    "100 medical articles\n",
    "100 politics articles\n",
    "100 space articles\n",
    "100 sport articles\n",
    "100 technologies articles\n",
    "\n",
    "These are to be in the same folder. The folder below is from the writer's\n",
    "personal desktop account, please change it accordingly to suit the user's\n",
    "needs and correct pathways.\n",
    "\"\"\"\n",
    "\n",
    "# Using the library pathlib from Path to read the files in the correct\n",
    "# folder - this is only to begin reading files\n",
    "from pathlib import Path\n",
    "\n",
    "# Create an object 'database' to link to the correct folder\n",
    "database = Path(\"C:\\\\Users\\\\stanleytjandra.DESKTOP-OOIPU77\\\\Desktop\\\\Part B\")\n",
    "\n",
    "# Set up an emptiy dictionary to populate later. The dictionary should have\n",
    "# the following format: {keys (doc title) : values (content of doc)}\n",
    "articles_dict = {}\n",
    "\n",
    "# Iterate through all .txt files using .glob and .stem\n",
    "for txt_file in database.glob('*.txt'):    #.glob() is a method that returns all file paths that matches with .txt\n",
    "    file_key = txt_file.stem    #.stem gives the file name without the extension\n",
    "    with txt_file.open('r', encoding='utf-8') as content:    # use utf-08 to read all characters (just in case there are weird characters)\n",
    "        txt_content = content.read()    # Read all files\n",
    "        txt_content = txt_content.replace('\\n', ' ')    # Replace whitespace with space\n",
    "        articles_dict[file_key] = txt_content    # Add all contents of file as dictionary values\n",
    "\n",
    "# Print how many articles are there to check:\n",
    "print(f'Total articles imported: {len(articles_dict)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e826bb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_1: Lufthansa flies back to profit...\n",
      "business_10: Winn-Dixie files for bankruptc...\n",
      "business_100: US economy still growing says ...\n",
      "business_11: Saab to build Cadillacs in Swe...\n",
      "business_12: Bank voted 8-1 for no rate cha...\n",
      "business_13: Industrial revival hope for Ja...\n",
      "business_14: Khodorkovsky ally denies charg...\n",
      "business_15: China keeps tight rein on cred...\n",
      "business_16: Verizon 'seals takeover of MCI...\n",
      "business_17: Crossrail link 'to get go-ahea...\n",
      "business_18: Small firms 'hit by rising cos...\n",
      "business_19: Deutsche Boerse boosts dividen...\n",
      "business_2: Japanese growth grinds to a ha...\n",
      "business_20: Brewers' profits lose their fi...\n",
      "business_21: Russia WTO talks 'make progres...\n",
      "business_22: India's rupee hits five-year h...\n",
      "business_23: Dollar drops on reserves conce...\n",
      "business_24: India and Russia in energy tal...\n",
      "business_25: Weak data buffets French econo...\n",
      "business_26: Business fears over sluggish E...\n"
     ]
    }
   ],
   "source": [
    "# Test that the dictionary article_list is correctly imported\n",
    "# Check for the first 20 articles and the first 30 characters\n",
    "for key in list(articles_dict.keys())[:20]:\n",
    "    print(f'{key}: {articles_dict[key][:30]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab721808",
   "metadata": {},
   "source": [
    "Confirmed that the dictionary is uploaded correctly. Since business articles are the top of the list, the system iterates top-bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4547a74c",
   "metadata": {},
   "source": [
    "##### Part 2: Tokenization, Stopwords Removal and Stemmer (Porter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68936f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This section attempts to preprocess words in preparation for TF-IDF scoring\n",
    "in the next section. Natural Language ToolKit library is used to conduct\n",
    "word tokenization, and also stopwrods removal and PorterStemmer functions.\n",
    "\"\"\"\n",
    "\n",
    "# Import library nltk and its related functions\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Prepare functions\n",
    "stemmer = PorterStemmer()    # PorterStemmer is chosen for robustness\n",
    "stop_words = set(stopwords.words('english'))    # All articles are in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b847377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that does three functions: tokenize, stopwords removal\n",
    "# and stem all words\n",
    "\n",
    "def preprocess_txt(i):\n",
    "    \n",
    "    tokens = word_tokenize(i)\n",
    "    \n",
    "    preprocessed_tokens = []    # Set up a list for all preprocessed word tokens to populate\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = token.lower()    # All preprocessing needs to be in lowercase\n",
    "        \n",
    "        if token not in stop_words and token.isalpha():    # Remove all english stop words and word tokens must be alphanumeric\n",
    "            stemmedtokens = stemmer.stem(token)    # Use Porter Stemmer algorithm\n",
    "            preprocessed_tokens.append(stemmedtokens)    # Populate all preprocessed word tokens to poopulate the list\n",
    "    \n",
    "    return preprocessed_tokens\n",
    "\n",
    "# Set up an empty dictionary to connect keys and values (the values being\n",
    "# stemmed tokens related to their article titles - \"keys\")\n",
    "preprocessed_articles = {}\n",
    "\n",
    "# Then populate the dictionary\n",
    "for key, txt_content in articles_dict.items():\n",
    "    preprocessed_articles[key] = preprocess_txt(txt_content)    # Use the function above to populate the dictionary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50fc1ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "business_1: ['lufthansa', 'fli', 'back', 'profit', 'german', 'airlin', 'lufthansa', 'return', 'profit', 'post', 'huge', 'loss', 'preliminari', 'report', 'airlin', 'announc', 'net', 'profit', 'euro', 'compar', 'loss', 'euro', 'oper', 'profit', 'euro']...\n",
      "business_10: ['file', 'bankruptci', 'us', 'supermarket', 'group', 'file', 'bankruptci', 'protect', 'succumb', 'stiff', 'competit', 'market', 'domin', 'among', 'profit', 'us', 'grocer', 'said', 'chapter', 'protect', 'would', 'enabl', 'success', 'restructur', 'said']...\n",
      "business_100: ['us', 'economi', 'still', 'grow', 'say', 'fed', 'area', 'us', 'saw', 'economi', 'continu', 'expand', 'decemb', 'earli', 'januari', 'us', 'feder', 'reserv', 'said', 'latest', 'beig', 'book', 'report', 'us', 'region']...\n",
      "business_11: ['saab', 'build', 'cadillac', 'sweden', 'gener', 'motor', 'world', 'largest', 'car', 'maker', 'confirm', 'build', 'new', 'cadillac', 'bl', 'saab', 'factori', 'sweden', 'car', 'unveil', 'geneva', 'motor', 'show', 'intend', 'compet']...\n",
      "business_12: ['bank', 'vote', 'rate', 'chang', 'decis', 'keep', 'interest', 'rate', 'hold', 'earlier', 'month', 'pass', 'bank', 'england', 'bodi', 'minut', 'shown', 'one', 'member', 'bank', 'monetari', 'polici', 'committe', 'mpc', 'paul']...\n",
      "business_13: ['industri', 'reviv', 'hope', 'japan', 'japanes', 'industri', 'grow', 'faster', 'expect', 'boost', 'hope', 'countri', 'retreat', 'back', 'recess', 'industri', 'output', 'rose', 'adjust', 'time', 'year', 'januari', 'month', 'earlier', 'time']...\n",
      "business_14: ['khodorkovski', 'alli', 'deni', 'charg', 'close', 'associ', 'former', 'yuko', 'boss', 'mikhail', 'khodorkovski', 'told', 'court', 'fraud', 'charg', 'level', 'fals', 'platon', 'lebedev', 'trial', 'alongsid', 'mr', 'khodorkovski', 'sinc', 'june']...\n",
      "business_15: ['china', 'keep', 'tight', 'rein', 'credit', 'china', 'effort', 'stop', 'economi', 'overh', 'clamp', 'credit', 'continu', 'state', 'media', 'report', 'curb', 'introduc', 'earlier', 'year', 'ward', 'risk', 'rapid', 'expans', 'might']...\n",
      "business_16: ['verizon', 'takeov', 'mci', 'verizon', 'takeov', 'battl', 'us', 'phone', 'firm', 'mci', 'bid', 'worth', 'report', 'say', 'two', 'firm', 'expect', 'seal', 'deal', 'monday', 'morn', 'accord', 'news', 'agenc', 'report']...\n",
      "business_17: ['crossrail', 'link', 'get', 'crossrail', 'transport', 'plan', 'back', 'busi', 'group', 'get', 'month', 'accord', 'mail', 'sunday', 'say', 'uk', 'treasuri', 'alloc', 'project', 'talk', 'busi', 'group', 'rais', 'rest', 'begin']...\n",
      "business_18: ['small', 'firm', 'rise', 'cost', 'rise', 'fuel', 'materi', 'cost', 'hit', 'confid', 'among', 'uk', 'small', 'manufactur', 'despit', 'rise', 'output', 'busi', 'lobbi', 'group', 'cbi', 'say', 'cbi', 'quarterli', 'survey']...\n",
      "business_19: ['deutsch', 'boers', 'boost', 'dividend', 'deutsch', 'boers', 'german', 'stock', 'exchang', 'tri', 'buy', 'london', 'rival', 'said', 'boost', 'dividend', 'payment', 'analyst', 'said', 'move', 'aim', 'win', 'investor', 'oppos', 'bid']...\n",
      "business_2: ['japanes', 'growth', 'grind', 'halt', 'growth', 'japan', 'evapor', 'three', 'month', 'septemb', 'spark', 'renew', 'concern', 'economi', 'long', 'trough', 'output', 'period', 'grew', 'annual', 'rate', 'export', 'usual', 'engin', 'recoveri']...\n",
      "business_20: ['brewer', 'profit', 'lose', 'fizz', 'heineken', 'carlsberg', 'two', 'world', 'largest', 'brewer', 'report', 'fall', 'profit', 'beer', 'sale', 'western', 'europ', 'fell', 'flat', 'dutch', 'firm', 'heineken', 'saw', 'annual', 'profit']...\n",
      "business_21: ['russia', 'wto', 'talk', 'progress', 'talk', 'russia', 'propos', 'membership', 'world', 'trade', 'organis', 'wto', 'make', 'good', 'progress', 'say', 'behind', 'negoti', 'chairman', 'work', 'parti', 'ambassador', 'stefan', 'johannesson', 'iceland']...\n"
     ]
    }
   ],
   "source": [
    "# Check if the preprocessed content is well-prepared\n",
    "for key in list(preprocessed_articles.keys())[:15]:    # Check the first 15 articles, values will be shown as a list\n",
    "    print(f'{key}: {preprocessed_articles[key][:25]}...')    # Check the first 25 word tokens of each article"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016c0ee6",
   "metadata": {},
   "source": [
    "As seen in the above dictionary, word tokens have been stemmed and linked back to their article titles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f866a6",
   "metadata": {},
   "source": [
    "##### Part 3: TF-IDF Weighting All Words Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fb2045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This sections intends to give array weightings to all word tokens based on\n",
    "their article title. Due to the nature of sklearn's TfidfVectorizer function,\n",
    "first all of the word tokens must be joined as a single string in each\n",
    "article. Then using the TfidfVectorizer's fit_transform() function to work\n",
    "out each token weighting. To confirm, the tf-idf vocabulary can be checked.\n",
    "The outcome of this is a .toarray() matrix of each word token weighting,\n",
    "however, due to the vast amount of text data, the array cannot be fully\n",
    "displayed on a screen.\n",
    "\"\"\"\n",
    "\n",
    "# First convert preprocessed articles values to strings\n",
    "txt_str = {}    # Create a new dictionary to be populated by list of strings\n",
    "\n",
    "for key, tokens in preprocessed_articles.items():    # Extract from previous dictionary\n",
    "    txt_str[key] = ' '.join(tokens)    # Values in txt_str dictionary are a list of strings of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958a652d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf-Idf representation:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "Text vocabulary (word tokens): ['aa' 'aaa' 'aaaa' 'aac' 'aan' 'aanerud' 'aangegeven' 'aantal' 'aao'\n",
      " 'aaron' 'aarseth' 'aavso' 'ab' 'abacu' 'abadan' 'aban' 'abandon' 'abat'\n",
      " 'abba' 'abbasi' 'abbey' 'abbott' 'abbrevi' 'abc' 'abd' 'abdel'\n",
      " 'abdelaziz' 'abdelhafid' 'abdic' 'abdomen' 'abdomin' 'abdullah' 'abeb'\n",
      " 'abel' 'aberr' 'aberystwyth' 'abeyi' 'abfp' 'abhin' 'abhorr' 'abi' 'abid'\n",
      " 'abil' 'abington' 'abiyot' 'abl' 'ablaz' 'abn' 'abner' 'abnorm' 'aboard'\n",
      " 'abolish' 'abort' 'abortionist' 'aboukir' 'abound' 'abraham' 'abraxi'\n",
      " 'abroad' 'abrog' 'abrupt' 'abruptli' 'absenc' 'absent' 'absentia'\n",
      " 'absolut' 'absorb' 'absorbt' 'absorpt' 'abstact' 'abstain' 'abstract'\n",
      " 'absurd' 'abtahi' 'abu' 'abund' 'abundantli' 'abus' 'abut' 'abuzz'\n",
      " 'abydo' 'abysm' 'abyss' 'ac' 'academ' 'academi' 'academia' 'acceler'\n",
      " 'accept' 'access' 'accessori' 'accid' 'accident' 'acclaim' 'acclim'\n",
      " 'accolad' 'accommod' 'accompani' 'accomplish' 'accord']...\n"
     ]
    }
   ],
   "source": [
    "# Import sklearn library TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Prepare the function\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Calculate the tfidf weighting values for each value token in each article\n",
    "tfidf_matrix = vectorizer.fit_transform(txt_str.values())\n",
    "\n",
    "# Show the array of weighting (not able to display all to to vast data)\n",
    "tfidf_array = tfidf_matrix.toarray()\n",
    "print(\"Tf-Idf representation:\\n\", tfidf_array)\n",
    "\n",
    "# Just to check for word tokens in the tf-idf matrix\n",
    "tfidf_vocabulary = vectorizer.get_feature_names_out()\n",
    "print(f'Text vocabulary (word tokens): {tfidf_vocabulary[:100]}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9fb948",
   "metadata": {},
   "source": [
    "As seen above, due to processing 1000 articles, a vast amount of text data makes it difficult to display tens of thousands of word tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3cbae7",
   "metadata": {},
   "source": [
    "##### Part 4: Query Function, Relevance Scoring and Ranking Retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "082a9980",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In order to calculate the relevance and similairty between query content\n",
    "and the tokens in the database, the library cosine_similarity is used.\n",
    "But before any cosine similarity calculations are computed, first the\n",
    "query content must also go through preprocessing algorithms similar to\n",
    "the word tokens in the database - to ensure consistency.\n",
    "Then the query goes through the vectorization using Tfidf library's\n",
    ".transfor() function to prepare for cosine_similarity\n",
    "\n",
    "Three functions are written here:\n",
    "- preprocess_query() is to preprocess (tokenize, remove stopwords, stem)\n",
    "- tfidf_query() is to vectorize the query tokens, and compute similarities\n",
    "- rank_relevant_results() is to sort cosine_similarities value, and order \n",
    "hem in ascending order based on the values, which will be a way of ranking\n",
    "the retrieval results of the query (using numpy library's argsort function)\n",
    "\n",
    "For the purposes of this assignment, the relevance scores of each result\n",
    "is displayed (higher cosine_similarity score means higher rank)\n",
    "\n",
    "One issue in this project is that there may need to be more than top 3\n",
    "results to analyze the results better. Since each article type consists of\n",
    "100 articles, this project will show the top 100 results.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def preprocess_query(query):\n",
    "    \n",
    "    # Word tokenize all query content\n",
    "    tokens = word_tokenize(query)\n",
    "    \n",
    "    # Stopword removal and stem tokens as previously in the database\n",
    "    preprocessed_query_tokens = []    # Set up a list for all preprocessed word tokens to populate\n",
    "    \n",
    "    for token in tokens:\n",
    "        token = token.lower()    # All preprocessing needs to be in lowercase\n",
    "        \n",
    "        if token not in stop_words and token.isalpha():    # Remove all english stop words and word tokens must be alphanumeric\n",
    "            stemmedtokens = stemmer.stem(token)    # Use Porter Stemmer algorithm\n",
    "            preprocessed_query_tokens.append(stemmedtokens)    # Populate all preprocessed word tokens to poopulate the list\n",
    "    \n",
    "    preprocessed_query_tokens = ' '.join(preprocessed_query_tokens)    # In one function, prepare query tokens to be vectorized\n",
    "\n",
    "    return preprocessed_query_tokens\n",
    "\n",
    "\n",
    "def tfidf_query(query, vectorizer, tfidf_matrix, n=100):\n",
    "    \n",
    "    # Preprocess query content:\n",
    "    tokenized_query = preprocess_query(query)\n",
    "    \n",
    "    # Transform query content using Tfidf vectorizer (.transform() function)\n",
    "    vectorized_query = vectorizer.transform([tokenized_query])\n",
    "    \n",
    "    # Compute cosine similarities with documents in the database\n",
    "    # .flatten() reduces the array to 1-dimension, which is useful in calculating cosine similarities\n",
    "    cosine_similarities = cosine_similarity(vectorized_query, tfidf_matrix).flatten()\n",
    "    \n",
    "    return cosine_similarities\n",
    "\n",
    "\n",
    "def rank_relevant_results(cosine_similarities, n=100):\n",
    "    \n",
    "    # Rank results based on cosine_similarity values using np.argsort()\n",
    "    top_hundred_relevance = np.argsort(cosine_similarities)[::-1][:n]\n",
    "    \n",
    "    # Based on the ranks in the top 100 results, list all the article titles\n",
    "    top_hundred_articles = [(list(txt_str.keys())[rank], cosine_similarities[rank]) for rank in top_hundred_relevance]\n",
    "    \n",
    "    return top_hundred_articles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dad626",
   "metadata": {},
   "source": [
    "The three functions above will be called in one single function below. The function query_retrieval prompts users to enter search queries and the top 100 results will be listed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba9766ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The function below aims to input search queries and run the search query\n",
    "through the previous three functions to vectorize, compute similarities to\n",
    "the database vectors, and display a ranked retrieval result to the user.\n",
    "\"\"\"\n",
    "\n",
    "def query_retrieval():\n",
    "    # Input prompt for users\n",
    "    while True:\n",
    "        \n",
    "        query = input(\"Enter your search terms here (or type 'end' to exit):\")\n",
    "        \n",
    "        if query.lower() == 'end':\n",
    "            break\n",
    "        \n",
    "        # Recall tfidf_query function above (including preprocess_uery function)\n",
    "        top_cosine = tfidf_query(query, vectorizer, tfidf_matrix, n=100)\n",
    "        \n",
    "        #Recall rank_relevant_results function above\n",
    "        top_relevant_articles = rank_relevant_results(top_cosine, n=100)\n",
    "        \n",
    "        # Print the top 100 relevant articles and their relevant scores\n",
    "        for rank, (title, score) in enumerate(top_relevant_articles, start=1):    # enumerate is used to provide rank number based on the cosine similarity values\n",
    "            print(f'Article {rank}: {title}, Relevance: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5065c0ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search terms here (or type 'end' to exit):global economic prediction of the future\n",
      "Article 1: business_48, Relevance: 0.15981872988731097\n",
      "Article 2: business_95, Relevance: 0.11884338918950614\n",
      "Article 3: business_56, Relevance: 0.1180562549235363\n",
      "Article 4: politics_128, Relevance: 0.11640131812131319\n",
      "Article 5: politics_21, Relevance: 0.11546592399094732\n",
      "Article 6: business_71, Relevance: 0.11297651202713817\n",
      "Article 7: business_96, Relevance: 0.10540175352589681\n",
      "Article 8: politics_15, Relevance: 0.10323224801916313\n",
      "Article 9: business_79, Relevance: 0.09581502925664231\n",
      "Article 10: historical_89, Relevance: 0.087429942392281\n",
      "Article 11: historical_27, Relevance: 0.08648856944078843\n",
      "Article 12: business_29, Relevance: 0.08029804731964885\n",
      "Article 13: politics_189, Relevance: 0.07604991694251638\n",
      "Article 14: space_47, Relevance: 0.07460986811112091\n",
      "Article 15: technologie_82, Relevance: 0.0742622290024628\n",
      "Article 16: medical_145, Relevance: 0.0700790680552658\n",
      "Article 17: sport_98, Relevance: 0.06984166678134912\n",
      "Article 18: politics_153, Relevance: 0.06927346064081634\n",
      "Article 19: business_43, Relevance: 0.06867186588115393\n",
      "Article 20: space_61, Relevance: 0.06697080172363624\n",
      "Article 21: business_82, Relevance: 0.06662499002434258\n",
      "Article 22: business_53, Relevance: 0.06662499002434258\n",
      "Article 23: technologie_72, Relevance: 0.06371502469902127\n",
      "Article 24: space_56, Relevance: 0.06346846027175562\n",
      "Article 25: technologie_16, Relevance: 0.05933959667826163\n",
      "Article 26: business_23, Relevance: 0.05758880814331132\n",
      "Article 27: politics_321, Relevance: 0.05699680221055791\n",
      "Article 28: politics_217, Relevance: 0.05699680221055791\n",
      "Article 29: technologie_45, Relevance: 0.05289827617461865\n",
      "Article 30: business_7, Relevance: 0.05099487464346131\n",
      "Article 31: space_24, Relevance: 0.05088782904524505\n",
      "Article 32: business_68, Relevance: 0.04840971850529265\n",
      "Article 33: business_42, Relevance: 0.047637916992730865\n",
      "Article 34: historical_46, Relevance: 0.0473256311642641\n",
      "Article 35: sport_67, Relevance: 0.04725703380492627\n",
      "Article 36: technologie_46, Relevance: 0.04653197848361997\n",
      "Article 37: business_6, Relevance: 0.04528938343746636\n",
      "Article 38: technologie_29, Relevance: 0.04511274346290229\n",
      "Article 39: business_86, Relevance: 0.044739494054942146\n",
      "Article 40: business_49, Relevance: 0.044581282330697095\n",
      "Article 41: politics_130, Relevance: 0.044320648876312214\n",
      "Article 42: politics_282, Relevance: 0.04388947883725592\n",
      "Article 43: technologie_41, Relevance: 0.041190860415959596\n",
      "Article 44: technologie_78, Relevance: 0.04082059490123293\n",
      "Article 45: medical_376, Relevance: 0.04010006727295633\n",
      "Article 46: technologie_59, Relevance: 0.039863061026517044\n",
      "Article 47: graphics_13, Relevance: 0.03959107435554837\n",
      "Article 48: politics_220, Relevance: 0.03950956436094523\n",
      "Article 49: business_4, Relevance: 0.03834276978423996\n",
      "Article 50: historical_17, Relevance: 0.03750883120609491\n",
      "Article 51: politics_127, Relevance: 0.03717306284285629\n",
      "Article 52: graphics_66, Relevance: 0.03609327471911303\n",
      "Article 53: technologie_17, Relevance: 0.03589957521215282\n",
      "Article 54: business_35, Relevance: 0.03537222181199103\n",
      "Article 55: technologie_50, Relevance: 0.035079450779188365\n",
      "Article 56: business_90, Relevance: 0.034872832823127364\n",
      "Article 57: politics_272, Relevance: 0.03482940613527337\n",
      "Article 58: space_9, Relevance: 0.034408352137486053\n",
      "Article 59: business_57, Relevance: 0.03383444090690424\n",
      "Article 60: business_100, Relevance: 0.0337847553378929\n",
      "Article 61: technologie_11, Relevance: 0.03371187572849873\n",
      "Article 62: historical_22, Relevance: 0.033333111206031454\n",
      "Article 63: historical_25, Relevance: 0.03327773297492923\n",
      "Article 64: historical_43, Relevance: 0.033253715955691356\n",
      "Article 65: entertainment_64, Relevance: 0.03278717798909664\n",
      "Article 66: business_9, Relevance: 0.03256368104973609\n",
      "Article 67: food_79, Relevance: 0.03226069021082631\n",
      "Article 68: historical_37, Relevance: 0.03213569499397109\n",
      "Article 69: space_67, Relevance: 0.03130071527193655\n",
      "Article 70: business_67, Relevance: 0.03119228183375066\n",
      "Article 71: business_2, Relevance: 0.031164583053914684\n",
      "Article 72: technologie_69, Relevance: 0.031136887476381526\n",
      "Article 73: entertainment_100, Relevance: 0.031093429415787934\n",
      "Article 74: business_62, Relevance: 0.030913875470303928\n",
      "Article 75: business_26, Relevance: 0.030777685585819406\n",
      "Article 76: business_15, Relevance: 0.029852621600722645\n",
      "Article 77: food_43, Relevance: 0.029338880590267846\n",
      "Article 78: sport_21, Relevance: 0.029325058155629496\n",
      "Article 79: entertainment_7, Relevance: 0.029316438624287712\n",
      "Article 80: business_13, Relevance: 0.02913482645576285\n",
      "Article 81: technologie_93, Relevance: 0.028816159509889742\n",
      "Article 82: entertainment_45, Relevance: 0.02878460874620857\n",
      "Article 83: business_20, Relevance: 0.028698491505383224\n",
      "Article 84: business_91, Relevance: 0.028582026209945124\n",
      "Article 85: technologie_64, Relevance: 0.028226142495744122\n",
      "Article 86: historical_3, Relevance: 0.0276522680256258\n",
      "Article 87: politics_301, Relevance: 0.027651315683048985\n",
      "Article 88: business_38, Relevance: 0.02762830492681497\n",
      "Article 89: technologie_31, Relevance: 0.026986357664595582\n",
      "Article 90: technologie_87, Relevance: 0.02641029759392375\n",
      "Article 91: business_50, Relevance: 0.02619008380229917\n",
      "Article 92: space_27, Relevance: 0.026160773684333116\n",
      "Article 93: business_51, Relevance: 0.025967600666338166\n",
      "Article 94: business_39, Relevance: 0.025959206447524096\n",
      "Article 95: entertainment_20, Relevance: 0.025684875147238155\n",
      "Article 96: space_77, Relevance: 0.025671489227662995\n",
      "Article 97: business_24, Relevance: 0.025640660512557104\n",
      "Article 98: technologie_74, Relevance: 0.02552671968273746\n",
      "Article 99: business_80, Relevance: 0.02531558591166187\n",
      "Article 100: graphics_16, Relevance: 0.025195279235322054\n",
      "Enter your search terms here (or type 'end' to exit):end\n"
     ]
    }
   ],
   "source": [
    "# Trial query 1: search for 'global economic prediction of the future'\n",
    "query_retrieval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84c8ced",
   "metadata": {},
   "source": [
    "As expected, many of the search results turned out to be from articles related to business, politics and to a certain extent, historical and techologies. The IR has somewhat captured the main intention of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8bda1f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search terms here (or type 'end' to exit):athlete performance improvements in football\n",
      "Article 1: sport_43, Relevance: 0.22714970579762211\n",
      "Article 2: sport_46, Relevance: 0.2019393184742041\n",
      "Article 3: sport_15, Relevance: 0.19117073944066915\n",
      "Article 4: sport_27, Relevance: 0.17908106019875208\n",
      "Article 5: sport_4, Relevance: 0.16966070624273114\n",
      "Article 6: sport_16, Relevance: 0.1452495938960811\n",
      "Article 7: sport_80, Relevance: 0.14456479318749943\n",
      "Article 8: sport_30, Relevance: 0.1081868390021885\n",
      "Article 9: medical_329, Relevance: 0.09738199845427029\n",
      "Article 10: entertainment_4, Relevance: 0.0962061055882629\n",
      "Article 11: sport_81, Relevance: 0.0895831963745092\n",
      "Article 12: sport_92, Relevance: 0.08844889996298112\n",
      "Article 13: sport_54, Relevance: 0.08843056317775762\n",
      "Article 14: sport_59, Relevance: 0.0877641305717896\n",
      "Article 15: medical_318, Relevance: 0.08736088595881253\n",
      "Article 16: sport_45, Relevance: 0.08670311458892915\n",
      "Article 17: entertainment_26, Relevance: 0.08160098680077545\n",
      "Article 18: sport_37, Relevance: 0.08041713017788135\n",
      "Article 19: business_86, Relevance: 0.07748743588503917\n",
      "Article 20: food_51, Relevance: 0.0751121401853451\n",
      "Article 21: sport_57, Relevance: 0.07501873129053609\n",
      "Article 22: sport_31, Relevance: 0.07403227615113012\n",
      "Article 23: sport_23, Relevance: 0.07323802529070919\n",
      "Article 24: sport_50, Relevance: 0.07041968080651836\n",
      "Article 25: sport_33, Relevance: 0.06993923546068523\n",
      "Article 26: sport_17, Relevance: 0.06989492516031388\n",
      "Article 27: sport_21, Relevance: 0.06892957632151726\n",
      "Article 28: entertainment_21, Relevance: 0.06730821406547323\n",
      "Article 29: sport_19, Relevance: 0.06705693277082336\n",
      "Article 30: sport_35, Relevance: 0.06691656226310017\n",
      "Article 31: sport_34, Relevance: 0.06489683495687171\n",
      "Article 32: entertainment_39, Relevance: 0.0638640489529171\n",
      "Article 33: sport_26, Relevance: 0.06113154035495762\n",
      "Article 34: sport_41, Relevance: 0.060827622493199854\n",
      "Article 35: sport_40, Relevance: 0.059884322007294016\n",
      "Article 36: sport_71, Relevance: 0.05916571839294278\n",
      "Article 37: sport_11, Relevance: 0.055088823455378216\n",
      "Article 38: entertainment_57, Relevance: 0.0532682248761994\n",
      "Article 39: entertainment_52, Relevance: 0.053046250922928126\n",
      "Article 40: sport_39, Relevance: 0.052332442698394405\n",
      "Article 41: sport_58, Relevance: 0.049649265286885146\n",
      "Article 42: sport_62, Relevance: 0.04791525175383981\n",
      "Article 43: sport_90, Relevance: 0.04644727765559912\n",
      "Article 44: sport_89, Relevance: 0.04385226212922204\n",
      "Article 45: sport_66, Relevance: 0.043674864679570144\n",
      "Article 46: business_13, Relevance: 0.04355500908451608\n",
      "Article 47: sport_100, Relevance: 0.043178415346276464\n",
      "Article 48: entertainment_23, Relevance: 0.04206476400829895\n",
      "Article 49: business_92, Relevance: 0.04168617175482392\n",
      "Article 50: space_30, Relevance: 0.0410217221617398\n",
      "Article 51: sport_84, Relevance: 0.04065501676681878\n",
      "Article 52: sport_52, Relevance: 0.04065501676681878\n",
      "Article 53: entertainment_69, Relevance: 0.040162582573511865\n",
      "Article 54: historical_31, Relevance: 0.0392680063213537\n",
      "Article 55: politics_283, Relevance: 0.03862766129366673\n",
      "Article 56: sport_13, Relevance: 0.03791817171788809\n",
      "Article 57: technologie_3, Relevance: 0.03744810065343776\n",
      "Article 58: technologie_36, Relevance: 0.03744810065343776\n",
      "Article 59: entertainment_45, Relevance: 0.03656350405308805\n",
      "Article 60: business_94, Relevance: 0.0362097533383011\n",
      "Article 61: food_64, Relevance: 0.03618220660607063\n",
      "Article 62: entertainment_51, Relevance: 0.03617249841004885\n",
      "Article 63: technologie_54, Relevance: 0.03600170946436362\n",
      "Article 64: sport_82, Relevance: 0.03582070035213615\n",
      "Article 65: entertainment_15, Relevance: 0.03575459319815741\n",
      "Article 66: sport_48, Relevance: 0.03566269537655297\n",
      "Article 67: sport_2, Relevance: 0.035413618378061236\n",
      "Article 68: graphics_94, Relevance: 0.0321679086197619\n",
      "Article 69: medical_346, Relevance: 0.03204386941600461\n",
      "Article 70: politics_138, Relevance: 0.031722940703918685\n",
      "Article 71: sport_86, Relevance: 0.030144872288076573\n",
      "Article 72: sport_94, Relevance: 0.029609503197627438\n",
      "Article 73: entertainment_17, Relevance: 0.02954969568500897\n",
      "Article 74: entertainment_70, Relevance: 0.027974635448972128\n",
      "Article 75: space_58, Relevance: 0.02794664266787672\n",
      "Article 76: entertainment_54, Relevance: 0.026689078762974733\n",
      "Article 77: sport_55, Relevance: 0.026539504285805694\n",
      "Article 78: food_69, Relevance: 0.026525472803428624\n",
      "Article 79: sport_87, Relevance: 0.02619036920291607\n",
      "Article 80: sport_8, Relevance: 0.026126616169214713\n",
      "Article 81: graphics_77, Relevance: 0.02597177153653356\n",
      "Article 82: business_100, Relevance: 0.02566317723863551\n",
      "Article 83: sport_1, Relevance: 0.025614643700357115\n",
      "Article 84: politics_180, Relevance: 0.025259810883061\n",
      "Article 85: sport_5, Relevance: 0.02461896975516134\n",
      "Article 86: business_56, Relevance: 0.024500899602231306\n",
      "Article 87: business_4, Relevance: 0.024352354947283078\n",
      "Article 88: sport_79, Relevance: 0.023283843567287108\n",
      "Article 89: sport_28, Relevance: 0.022494627197170727\n",
      "Article 90: sport_44, Relevance: 0.02236517021208796\n",
      "Article 91: business_78, Relevance: 0.022351334834482553\n",
      "Article 92: sport_69, Relevance: 0.02204710235164916\n",
      "Article 93: medical_1, Relevance: 0.02179502809831186\n",
      "Article 94: sport_51, Relevance: 0.021791136229395602\n",
      "Article 95: sport_63, Relevance: 0.02172833950527661\n",
      "Article 96: sport_99, Relevance: 0.021302802100973683\n",
      "Article 97: entertainment_49, Relevance: 0.021058658264410137\n",
      "Article 98: business_2, Relevance: 0.02091696818807413\n",
      "Article 99: business_71, Relevance: 0.020768991210236364\n",
      "Article 100: business_10, Relevance: 0.020522171309568727\n",
      "Enter your search terms here (or type 'end' to exit):end\n"
     ]
    }
   ],
   "source": [
    "# Trial query 2: search for 'athlete performance improvements in football'\n",
    "query_retrieval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee0cf04",
   "metadata": {},
   "source": [
    "The search query 2 above is more diverse as sport may be linked to entertainment, business and medical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23919cb0",
   "metadata": {},
   "source": [
    "##### Part 5: Evaluating the IR System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "351769f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search terms here (or type 'end' to exit):medical\n",
      "Article 1: medical_121, Relevance: 0.36153929624303915\n",
      "Article 2: medical_437, Relevance: 0.32813887217317683\n",
      "Article 3: medical_102, Relevance: 0.20783802781687508\n",
      "Article 4: medical_557, Relevance: 0.20143692673576621\n",
      "Article 5: medical_468, Relevance: 0.19096313408410406\n",
      "Article 6: medical_488, Relevance: 0.18831919202491498\n",
      "Article 7: medical_327, Relevance: 0.18646907227514922\n",
      "Article 8: medical_346, Relevance: 0.1688871996969261\n",
      "Article 9: medical_300, Relevance: 0.1484292364094448\n",
      "Article 10: medical_246, Relevance: 0.14424025071376423\n",
      "Article 11: medical_186, Relevance: 0.14123612081695086\n",
      "Article 12: medical_646, Relevance: 0.13937835533944043\n",
      "Article 13: medical_67, Relevance: 0.13725260252158109\n",
      "Article 14: medical_692, Relevance: 0.13659783558228641\n",
      "Article 15: medical_608, Relevance: 0.129131425800171\n",
      "Article 16: medical_319, Relevance: 0.1268857082332253\n",
      "Article 17: medical_244, Relevance: 0.12445777743885442\n",
      "Article 18: medical_103, Relevance: 0.12276473366466625\n",
      "Article 19: medical_531, Relevance: 0.1200191444998191\n",
      "Article 20: medical_485, Relevance: 0.11814821424261632\n",
      "Article 21: medical_695, Relevance: 0.11754239088450355\n",
      "Article 22: medical_458, Relevance: 0.1094596775351644\n",
      "Article 23: medical_529, Relevance: 0.10904344648657696\n",
      "Article 24: medical_645, Relevance: 0.10811696340086045\n",
      "Article 25: medical_466, Relevance: 0.10462436900086769\n",
      "Article 26: sport_33, Relevance: 0.1028882621827935\n",
      "Article 27: medical_360, Relevance: 0.10246341566488651\n",
      "Article 28: medical_289, Relevance: 0.10189825090774696\n",
      "Article 29: medical_286, Relevance: 0.10041012749895374\n",
      "Article 30: medical_349, Relevance: 0.09627858471629872\n",
      "Article 31: medical_572, Relevance: 0.09570298948455593\n",
      "Article 32: medical_152, Relevance: 0.09196880735034726\n",
      "Article 33: medical_542, Relevance: 0.09072763705849758\n",
      "Article 34: medical_294, Relevance: 0.08830658531629632\n",
      "Article 35: medical_376, Relevance: 0.08829798671119374\n",
      "Article 36: medical_487, Relevance: 0.08617101519091509\n",
      "Article 37: medical_40, Relevance: 0.08484826530974486\n",
      "Article 38: medical_248, Relevance: 0.08362684975014649\n",
      "Article 39: medical_624, Relevance: 0.08292205846732664\n",
      "Article 40: medical_644, Relevance: 0.07850109349317266\n",
      "Article 41: medical_564, Relevance: 0.07605954257189007\n",
      "Article 42: medical_58, Relevance: 0.07562140825585192\n",
      "Article 43: medical_560, Relevance: 0.07382596160133824\n",
      "Article 44: medical_250, Relevance: 0.07264560317066793\n",
      "Article 45: medical_406, Relevance: 0.07128909165226896\n",
      "Article 46: medical_308, Relevance: 0.07081167990516656\n",
      "Article 47: medical_134, Relevance: 0.06973460098715845\n",
      "Article 48: medical_424, Relevance: 0.06902125139996071\n",
      "Article 49: medical_694, Relevance: 0.06875103628712345\n",
      "Article 50: medical_413, Relevance: 0.06874245896611839\n",
      "Article 51: medical_382, Relevance: 0.061068273745843143\n",
      "Article 52: medical_176, Relevance: 0.057709753202110914\n",
      "Article 53: medical_586, Relevance: 0.057676769075613185\n",
      "Article 54: medical_401, Relevance: 0.056344764770341345\n",
      "Article 55: food_58, Relevance: 0.05505524490414517\n",
      "Article 56: medical_499, Relevance: 0.053359348857399874\n",
      "Article 57: medical_335, Relevance: 0.052014916964682474\n",
      "Article 58: entertainment_37, Relevance: 0.051215003547987956\n",
      "Article 59: medical_642, Relevance: 0.049230166519268634\n",
      "Article 60: food_64, Relevance: 0.04767457601034004\n",
      "Article 61: food_100, Relevance: 0.04491140131343762\n",
      "Article 62: medical_679, Relevance: 0.04242702685738194\n",
      "Article 63: politics_151, Relevance: 0.036526646781255195\n",
      "Article 64: historical_45, Relevance: 0.03523541620017225\n",
      "Article 65: medical_500, Relevance: 0.03212510158124727\n",
      "Article 66: food_87, Relevance: 0.02843039012459229\n",
      "Article 67: space_52, Relevance: 0.02766312133324345\n",
      "Article 68: food_86, Relevance: 0.02405387198275259\n",
      "Article 69: medical_438, Relevance: 0.02191504042131681\n",
      "Article 70: historical_6, Relevance: 0.021525928323091414\n",
      "Article 71: historical_78, Relevance: 0.020036023766372653\n",
      "Article 72: historical_1, Relevance: 0.016868227157295766\n",
      "Article 73: graphics_74, Relevance: 0.015628003661056135\n",
      "Article 74: historical_4, Relevance: 0.014399155264836271\n",
      "Article 75: graphics_30, Relevance: 0.0\n",
      "Article 76: graphics_32, Relevance: 0.0\n",
      "Article 77: graphics_31, Relevance: 0.0\n",
      "Article 78: technologie_99, Relevance: 0.0\n",
      "Article 79: graphics_33, Relevance: 0.0\n",
      "Article 80: graphics_56, Relevance: 0.0\n",
      "Article 81: graphics_50, Relevance: 0.0\n",
      "Article 82: graphics_51, Relevance: 0.0\n",
      "Article 83: graphics_52, Relevance: 0.0\n",
      "Article 84: graphics_53, Relevance: 0.0\n",
      "Article 85: graphics_54, Relevance: 0.0\n",
      "Article 86: graphics_55, Relevance: 0.0\n",
      "Article 87: graphics_57, Relevance: 0.0\n",
      "Article 88: graphics_34, Relevance: 0.0\n",
      "Article 89: graphics_58, Relevance: 0.0\n",
      "Article 90: graphics_59, Relevance: 0.0\n",
      "Article 91: graphics_6, Relevance: 0.0\n",
      "Article 92: graphics_60, Relevance: 0.0\n",
      "Article 93: graphics_61, Relevance: 0.0\n",
      "Article 94: graphics_62, Relevance: 0.0\n",
      "Article 95: graphics_5, Relevance: 0.0\n",
      "Article 96: graphics_49, Relevance: 0.0\n",
      "Article 97: graphics_48, Relevance: 0.0\n",
      "Article 98: graphics_47, Relevance: 0.0\n",
      "Article 99: graphics_46, Relevance: 0.0\n",
      "Article 100: graphics_45, Relevance: 0.0\n",
      "Enter your search terms here (or type 'end' to exit):end\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "To evaluate the IR system, first we use a search query example.\n",
    "\"\"\"\n",
    "\n",
    "# Trial query 2: search for 'medical'\n",
    "query_retrieval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "185e0f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                Predicted Positive   Predicted Negative\n",
      "Actual Positive    TP = 64                 FN = 36\n",
      "Actual Negative    FP = 10                 TN = 890\n",
      "\n",
      "Recall = 64.00%\n",
      "Precision = 86.49%\n",
      "F1-score = 73.56%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adapted from Module 5, the code below is written witht the intention to\n",
    "create a Confusion Matrix, and provide results for metrics such as Recall,\n",
    "Precision and F1 Score.\n",
    "\n",
    "This code analyzes the search query 'medical' above, which retrieves 74\n",
    "articles, with 64 relevant medical articles, but with a total relevant\n",
    "record of 100 medical articles in the database. These counts were done\n",
    "manually by the writer.\n",
    "\"\"\"\n",
    "\n",
    "# Given values\n",
    "total_relevant_records = 100\n",
    "retrieved_records = 74\n",
    "relevant_retrieved_records = 64\n",
    "\n",
    "# Calculate the number of relevant records not retrieved \n",
    "relevant_not_retrieved = total_relevant_records - relevant_retrieved_records\n",
    "\n",
    "# Calculate the number of irrelevant records retrieved \n",
    "irrelevant_retrieved = retrieved_records - relevant_retrieved_records\n",
    "\n",
    "# Confusion Matrix components\n",
    "TP = relevant_retrieved_records # True Positives : Number of relevant documents retrieved\n",
    "FN = relevant_not_retrieved # False Negatives : Number of relevant documents not retrieved\n",
    "FP = irrelevant_retrieved # False Positives : Number of irrelevant documents retrieved\n",
    "\n",
    "# Assuming total records in the database is 1000\n",
    "total_records = 1000\n",
    "\n",
    "# Calculate True Negatives (TN)\n",
    "TN = total_records - TP - FN - FP\n",
    "\n",
    "# Calculate recall\n",
    "recall = (TP / (TP + FN)) * 100\n",
    "\n",
    "# Calculate precision\n",
    "precision = (TP / (TP + FP)) * 100\n",
    "\n",
    "# Calculate F1-score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Display the confusion matrix and results\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(f\"                Predicted Positive   Predicted Negative\")\n",
    "print(f\"Actual Positive    TP = {TP}                 FN = {FN}\")\n",
    "print(f\"Actual Negative    FP = {FP}                 TN = {TN}\")\n",
    "\n",
    "print(f\"\\nRecall = {recall:.2f}%\")\n",
    "print(f\"Precision = {precision:.2f}%\")\n",
    "print(f\"F1-score = {f1_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c402de0f",
   "metadata": {},
   "source": [
    "Hence, for the search query 'medical' the IR system looks to quite reliable. One more search query 'history' is tested on the IR system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57721962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your search terms here (or type 'end' to exit):history\n",
      "Article 1: historical_69, Relevance: 0.29395066723774305\n",
      "Article 2: historical_14, Relevance: 0.2547101286339654\n",
      "Article 3: historical_75, Relevance: 0.11751422127418999\n",
      "Article 4: historical_11, Relevance: 0.11751422127418999\n",
      "Article 5: entertainment_49, Relevance: 0.0970021986364926\n",
      "Article 6: historical_72, Relevance: 0.08740403946857904\n",
      "Article 7: medical_13, Relevance: 0.08726601501575487\n",
      "Article 8: historical_65, Relevance: 0.0864842272899049\n",
      "Article 9: historical_83, Relevance: 0.08126581388069458\n",
      "Article 10: historical_99, Relevance: 0.08002056334256068\n",
      "Article 11: historical_5, Relevance: 0.0788452611660353\n",
      "Article 12: historical_22, Relevance: 0.07451770708234658\n",
      "Article 13: historical_47, Relevance: 0.07266774885258392\n",
      "Article 14: politics_128, Relevance: 0.0649294019435453\n",
      "Article 15: historical_80, Relevance: 0.06026484594401457\n",
      "Article 16: historical_59, Relevance: 0.05910362685176081\n",
      "Article 17: medical_401, Relevance: 0.05720467228492796\n",
      "Article 18: sport_42, Relevance: 0.0560364364821553\n",
      "Article 19: sport_40, Relevance: 0.05577725228327274\n",
      "Article 20: historical_32, Relevance: 0.051569209198722134\n",
      "Article 21: historical_78, Relevance: 0.05085451053527542\n",
      "Article 22: entertainment_3, Relevance: 0.049122895698556775\n",
      "Article 23: historical_90, Relevance: 0.04856641394499885\n",
      "Article 24: sport_41, Relevance: 0.048485779444219064\n",
      "Article 25: historical_46, Relevance: 0.04653407077704647\n",
      "Article 26: entertainment_30, Relevance: 0.04589202627057104\n",
      "Article 27: entertainment_33, Relevance: 0.045024842632366835\n",
      "Article 28: historical_4, Relevance: 0.04385672536545921\n",
      "Article 29: historical_6, Relevance: 0.043708893998239455\n",
      "Article 30: historical_64, Relevance: 0.04264466121527153\n",
      "Article 31: graphics_40, Relevance: 0.042172777864869095\n",
      "Article 32: entertainment_94, Relevance: 0.040635736382013884\n",
      "Article 33: historical_88, Relevance: 0.040568075741058085\n",
      "Article 34: historical_15, Relevance: 0.04006337482752924\n",
      "Article 35: medical_564, Relevance: 0.03861016391408159\n",
      "Article 36: historical_81, Relevance: 0.03785944789301651\n",
      "Article 37: politics_154, Relevance: 0.03751739063951771\n",
      "Article 38: historical_28, Relevance: 0.0373749612696759\n",
      "Article 39: historical_3, Relevance: 0.03664828794065879\n",
      "Article 40: space_73, Relevance: 0.03613015482763742\n",
      "Article 41: historical_1, Relevance: 0.034251324341982185\n",
      "Article 42: medical_500, Relevance: 0.03261537989493213\n",
      "Article 43: politics_182, Relevance: 0.03231944592340241\n",
      "Article 44: historical_82, Relevance: 0.03199101635768032\n",
      "Article 45: historical_96, Relevance: 0.03148982073044301\n",
      "Article 46: historical_73, Relevance: 0.0306219568750902\n",
      "Article 47: politics_282, Relevance: 0.029732301764535545\n",
      "Article 48: medical_586, Relevance: 0.02927850250925745\n",
      "Article 49: business_39, Relevance: 0.029016501492917044\n",
      "Article 50: politics_134, Relevance: 0.027412821085470673\n",
      "Article 51: business_51, Relevance: 0.025183376063858943\n",
      "Article 52: technologie_78, Relevance: 0.022814080532233647\n",
      "Article 53: historical_76, Relevance: 0.02116476227407895\n",
      "Article 54: technologie_1, Relevance: 0.020283507639381613\n",
      "Article 55: historical_33, Relevance: 0.018686080217900775\n",
      "Article 56: historical_71, Relevance: 0.01786706299287575\n",
      "Article 57: historical_29, Relevance: 0.016871660932566607\n",
      "Article 58: space_61, Relevance: 0.01679372636585345\n",
      "Article 59: historical_74, Relevance: 0.015123346301519614\n",
      "Article 60: historical_2, Relevance: 0.014931923961493421\n",
      "Article 61: historical_62, Relevance: 0.014803258891292427\n",
      "Article 62: space_11, Relevance: 0.0146871757262304\n",
      "Article 63: historical_86, Relevance: 0.012392945801134456\n",
      "Article 64: historical_34, Relevance: 0.011371734900469704\n",
      "Article 65: historical_42, Relevance: 0.011068932386646024\n",
      "Article 66: space_40, Relevance: 0.010473012874662657\n",
      "Article 67: historical_50, Relevance: 0.010289922354769077\n",
      "Article 68: medical_485, Relevance: 0.005452333507781139\n",
      "Article 69: space_54, Relevance: 0.005268846101027959\n",
      "Article 70: medical_487, Relevance: 0.004604532475010897\n",
      "Article 71: graphics_22, Relevance: 0.0\n",
      "Article 72: graphics_24, Relevance: 0.0\n",
      "Article 73: graphics_3, Relevance: 0.0\n",
      "Article 74: graphics_18, Relevance: 0.0\n",
      "Article 75: graphics_27, Relevance: 0.0\n",
      "Article 76: graphics_29, Relevance: 0.0\n",
      "Article 77: graphics_19, Relevance: 0.0\n",
      "Article 78: graphics_25, Relevance: 0.0\n",
      "Article 79: graphics_21, Relevance: 0.0\n",
      "Article 80: graphics_2, Relevance: 0.0\n",
      "Article 81: graphics_26, Relevance: 0.0\n",
      "Article 82: graphics_20, Relevance: 0.0\n",
      "Article 83: graphics_28, Relevance: 0.0\n",
      "Article 84: graphics_23, Relevance: 0.0\n",
      "Article 85: graphics_32, Relevance: 0.0\n",
      "Article 86: graphics_30, Relevance: 0.0\n",
      "Article 87: graphics_42, Relevance: 0.0\n",
      "Article 88: graphics_50, Relevance: 0.0\n",
      "Article 89: graphics_5, Relevance: 0.0\n",
      "Article 90: graphics_49, Relevance: 0.0\n",
      "Article 91: graphics_48, Relevance: 0.0\n",
      "Article 92: graphics_47, Relevance: 0.0\n",
      "Article 93: graphics_46, Relevance: 0.0\n",
      "Article 94: graphics_45, Relevance: 0.0\n",
      "Article 95: graphics_44, Relevance: 0.0\n",
      "Article 96: graphics_43, Relevance: 0.0\n",
      "Article 97: graphics_41, Relevance: 0.0\n",
      "Article 98: graphics_31, Relevance: 0.0\n",
      "Article 99: graphics_4, Relevance: 0.0\n",
      "Article 100: graphics_39, Relevance: 0.0\n",
      "Enter your search terms here (or type 'end' to exit):end\n"
     ]
    }
   ],
   "source": [
    "query_retrieval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "308ced4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                Predicted Positive   Predicted Negative\n",
      "Actual Positive    TP = 40                 FN = 60\n",
      "Actual Negative    FP = 30                 TN = 870\n",
      "\n",
      "Recall = 40.00%\n",
      "Precision = 57.14%\n",
      "F1-score = 47.06%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This code analyzes the search query 'history' above, which retrieves 70\n",
    "articles, with 40 relevant historical articles, but with a total relevant\n",
    "record of 100 historical articles in the database. These counts were done\n",
    "manually by the writer.\n",
    "\"\"\"\n",
    "\n",
    "# Given values\n",
    "total_relevant_records = 100\n",
    "retrieved_records = 70\n",
    "relevant_retrieved_records = 40\n",
    "\n",
    "# Calculate the number of relevant records not retrieved \n",
    "relevant_not_retrieved = total_relevant_records - relevant_retrieved_records\n",
    "\n",
    "# Calculate the number of irrelevant records retrieved \n",
    "irrelevant_retrieved = retrieved_records - relevant_retrieved_records\n",
    "\n",
    "# Confusion Matrix components\n",
    "TP = relevant_retrieved_records # True Positives : Number of relevant documents retrieved\n",
    "FN = relevant_not_retrieved # False Negatives : Number of relevant documents not retrieved\n",
    "FP = irrelevant_retrieved # False Positives : Number of irrelevant documents retrieved\n",
    "\n",
    "# Assuming total records in the database is total_relevant_records + total_irrelevant_records\n",
    "total_records = 1000\n",
    "\n",
    "# Calculate True Negatives (TN)\n",
    "TN = total_records - TP - FN - FP\n",
    "\n",
    "# Calculate recall\n",
    "recall = (TP / (TP + FN)) * 100\n",
    "\n",
    "# Calculate precision\n",
    "precision = (TP / (TP + FP)) * 100\n",
    "\n",
    "# Calculate F1-score\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Display the confusion matrix and results\n",
    "print(f\"Confusion Matrix:\")\n",
    "print(f\"                Predicted Positive   Predicted Negative\")\n",
    "print(f\"Actual Positive    TP = {TP}                 FN = {FN}\")\n",
    "print(f\"Actual Negative    FP = {FP}                 TN = {TN}\")\n",
    "\n",
    "print(f\"\\nRecall = {recall:.2f}%\")\n",
    "print(f\"Precision = {precision:.2f}%\")\n",
    "print(f\"F1-score = {f1_score:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a97b5",
   "metadata": {},
   "source": [
    "The result show a much reduced IR system reliability. This could possibly be due to the query being more diverse in the database, increasing the possibility of search 'hits' that are not exactly relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88220c11",
   "metadata": {},
   "source": [
    "##### Further Improvements\n",
    "\n",
    "Further improvements can be researched by:\n",
    "1. Using a different Stemmer (Lancaster or Snowball)\n",
    "2. Evaluating the IR System with more complex queries\n",
    "\n",
    "##### End of Assignment 3 Part B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
